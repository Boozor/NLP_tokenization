{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_tokenization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOVIw+FbgsafiA6fiDtDj+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoVal1/NLP_tokenization/blob/main/NLP_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFv-USWkhQKA"
      },
      "source": [
        "# Tokenizing text and creating sequences for sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mGaRDFcSamt"
      },
      "source": [
        "## Import the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EN1-FZodOuPl"
      },
      "outputs": [],
      "source": [
        "# Import the Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Qwn_7FSXW-"
      },
      "source": [
        "## Write some sentences\n",
        "\n",
        "Feel free to change and add sentences as you like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RMiq8BpWVVRa"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    'CSS stands for Cascading Style Sheets',\n",
        "    'CSS describes how HTML elements are to be displayed on screen, paper, or in other media',\n",
        "    'CSS saves a lot of work. It can control the layout of multiple web pages all at once',\n",
        "    'External stylesheets are stored in CSS files',\n",
        "    'In a programming language, variables are used to store data values.',\n",
        "    'JavaScript uses the var keyword to declare variables.',\n",
        "    'An equal sign is used to assign values to variables',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz845OtfRBCM"
      },
      "source": [
        "## Tokenize the words\n",
        "\n",
        "The first step to preparing text to be used in a machine learning model is to tokenize the text, in other words, to generate numbers for the words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZHTK1DAlQ1zO"
      },
      "outputs": [],
      "source": [
        "# Optionally set the max number of words to tokenize.\n",
        "# The out of vocabulary (OOV) token represents words that are not in the index.\n",
        "# Call fit_on_text() on the tokenizer to generate unique numbers for each word\n",
        "tokenizer = Tokenizer(num_words = 150, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mylv-WuiRzd0"
      },
      "source": [
        "## View the word index\n",
        "After you tokenize the text, the tokenizer has a word index that contains key-value pairs for all the words and their numbers.\n",
        "\n",
        "The word is the key, and the number is the value.\n",
        "\n",
        "Notice that the OOV token is the first entry.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kX4VvsLySC7Z",
        "outputId": "ee4826d4-15b2-4d35-e1c5-a14c0037a74d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'to': 2, 'css': 3, 'are': 4, 'in': 5, 'variables': 6, 'a': 7, 'of': 8, 'the': 9, 'used': 10, 'values': 11, 'stands': 12, 'for': 13, 'cascading': 14, 'style': 15, 'sheets': 16, 'describes': 17, 'how': 18, 'html': 19, 'elements': 20, 'be': 21, 'displayed': 22, 'on': 23, 'screen': 24, 'paper': 25, 'or': 26, 'other': 27, 'media': 28, 'saves': 29, 'lot': 30, 'work': 31, 'it': 32, 'can': 33, 'control': 34, 'layout': 35, 'multiple': 36, 'web': 37, 'pages': 38, 'all': 39, 'at': 40, 'once': 41, 'external': 42, 'stylesheets': 43, 'stored': 44, 'files': 45, 'programming': 46, 'language': 47, 'store': 48, 'data': 49, 'javascript': 50, 'uses': 51, 'var': 52, 'keyword': 53, 'declare': 54, 'an': 55, 'equal': 56, 'sign': 57, 'is': 58, 'assign': 59}\n"
          ]
        }
      ],
      "source": [
        "# Examine the word index\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JXKrGxsIVtLo",
        "outputId": "3deee9a9-65de-4cec-a6e7-3274b41be6c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43\n"
          ]
        }
      ],
      "source": [
        "# Get the number for a given word\n",
        "print(word_index['stylesheets'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcN_yM8O1oSX"
      },
      "source": [
        "# Create sequences for the sentences\n",
        "\n",
        "After you tokenize the words, the word index contains a unique number for each word. However, the numbers in the word index are not ordered. Words in a sentence have an order. So after tokenizing the words, the next step is to generate sequences for the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QlUL6Ybf1sso",
        "outputId": "397747e5-0dac-4279-b7e4-9b0831603d3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 12, 13, 14, 15, 16], [3, 17, 18, 19, 20, 4, 2, 21, 22, 23, 24, 25, 26, 5, 27, 28], [3, 29, 7, 30, 8, 31, 32, 33, 34, 9, 35, 8, 36, 37, 38, 39, 40, 41], [42, 43, 4, 44, 5, 3, 45], [5, 7, 46, 47, 6, 4, 10, 2, 48, 49, 11], [50, 51, 9, 52, 53, 2, 54, 6], [55, 56, 57, 58, 10, 2, 59, 11, 2, 6]]\n"
          ]
        }
      ],
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print (sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AswZPbuW8f-f"
      },
      "source": [
        "# Sequence sentences that contain words that are not in the word index\n",
        "\n",
        "Let's take a look at what happens if the sentence being sequenced contains words that are not in the word index.\n",
        "\n",
        "The Out of Vocabluary (OOV) token is the first entry in the word index. You will see it shows up in the sequences in place of any word that is not in the word index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fir7qd6X8eZc",
        "outputId": "4cc0e8c7-82b7-40f1-b66a-6312070bcd28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
          ]
        }
      ],
      "source": [
        "sentences2 = [\"I like hot chocolate\", \"My dogs and my hedgehog like kibble but my squirrel prefers grapes and my chickens like ice cream, preferably vanilla\"]\n",
        "\n",
        "sequences2 = tokenizer.texts_to_sequences(sentences2)\n",
        "print(sequences2)"
      ]
    }
  ]
}